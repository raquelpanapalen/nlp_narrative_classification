{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03d8abb8-7d1a-4152-b272-47d3cd4d172b",
   "metadata": {},
   "source": [
    "# Quantitative analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d194a6c-3112-4691-b040-14acf0ed3948",
   "metadata": {},
   "source": [
    "For a better intepretability, the quantitative analysis of the implemented models is divided into two macro-categories: Climate Change ( CC) and War in Ukraine (UA)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bf9a61-1083-4fcc-b6a9-22084b0694aa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Long short-term memory (CC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51894d5d-0e61-4e02-88fd-aa02ae991f34",
   "metadata": {},
   "source": [
    "**Overall performance**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409ecbd8-6c92-49b4-891c-b94424767cc3",
   "metadata": {},
   "source": [
    "- Accuracy: 0.0\n",
    "- F1 Score: 0.6172443732051025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b460c9-eff3-45d0-bbc2-1bb689c435b8",
   "metadata": {},
   "source": [
    "An accuracy of 0.0 in a model's performance indicates that the model did not correctly predict any instance in the dataset according to the true labels.\n",
    "The F1 score shows that the model achieves a moderate balance between precision  and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e338f08-7b1f-4ae3-a0e8-23d2fafdc991",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "**Performance Metrics Table**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e364f5-5cc5-48d2-b770-dbd239b87fbf",
   "metadata": {},
   "source": [
    "| Metric        | Precision | Recall | F1-Score | Support |\n",
    "|---------------|-----------|--------|----------|---------|\n",
    "| micro avg     | 0.28      | 0.68   | 0.40     | 31      |\n",
    "| macro avg     | 0.02      | 0.04   | 0.02     | 31      |\n",
    "| weighted avg  | 0.59      | 0.68   | 0.62     | 31      |\n",
    "| samples avg   | 0.31      | 0.92   | 0.45     | 31      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b038b7e0-c791-4ed4-90d5-df3b7a44b3be",
   "metadata": {},
   "source": [
    "The micro average shows moderate performance, with a precision of 0.28, recall of 0.68, and an F1-score of 0.40. This suggests the model is somewhat effective at identifying relevant instances but struggles to avoid false positives. The macro average indicates poor performance across classes, with very low precision, recall, and F1-score (all around 0.02). \n",
    "The weighted average provides the best results among the metrics, with precision at 0.59, recall at 0.68, and an F1-score of 0.62. This suggests the model performs well on classes with more instances, adjusting for class frequency. Lastly, the samples average reflects high recall (0.92) but lower precision (0.31) and an F1-score of 0.45, indicating that while the model captures most positive samples, it also misclassifies many instances as positive incorrectly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e31c90-08a0-4a2b-8ad0-1130da2ecf1b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Long short-term memory (UA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e7f392-043f-4557-9e57-b23b617b304d",
   "metadata": {},
   "source": [
    "**Overall performance**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51437e8d-f965-4a0c-ad0e-15ecd5b1bead",
   "metadata": {},
   "source": [
    "- Accuracy: 0.0\n",
    "- F1 Score: 0.12845804988662132 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b70fe14-0fb6-47d2-acf4-e2ac11645b9e",
   "metadata": {},
   "source": [
    "The model did not produce any correct prediction. The F1 score suggests a very low balance betweeen precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f743f21-8145-4e4a-8463-cf360d760969",
   "metadata": {},
   "source": [
    "**Performance Metrics Table**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85f79cc-b149-4115-b429-5deb2d106a99",
   "metadata": {},
   "source": [
    "| Metric        | Precision | Recall | F1-Score | Support |\n",
    "|---------------|-----------|--------|----------|---------|\n",
    "| micro avg     | 0.32      | 0.23   | 0.27     | 120     |\n",
    "| macro avg     | 0.02      | 0.04   | 0.02     | 120     |\n",
    "| weighted avg  | 0.09      | 0.23   | 0.13     | 120     |\n",
    "| samples avg   | 0.32      | 0.46   | 0.33     | 120     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844c5070-07da-4b10-9c2d-ae5f8d0506e0",
   "metadata": {},
   "source": [
    "The model's micro average F1-score is 0.27, indicating low performance when evaluating all instances equally. Precision is slightly higher at 0.32, but recall remains low at 0.23, suggesting the model struggles with identifying relevant instances. The macro average F1-score of 0.02 highlights severe difficulty in generalizing across all classes, particularly for less-represented ones. The weighted average F1-score of 0.13 reflects better performance on dominant classes, though still quite poor overall. At the sample level, the model achieves an F1-score of 0.33, with higher recall (0.46), showing it captures more positives but sacrifices precision. Overall, the model demonstrates hight limitations, particularly with class imbalances and underrepresented categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8051096f-8276-44d9-876b-95166727c908",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Support vector machine (CC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0de024-b535-4286-89d8-df9f67eb35b6",
   "metadata": {},
   "source": [
    "**Overall performance**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d8ecd6-2264-4a7b-afe8-10c5cc1e5045",
   "metadata": {},
   "source": [
    "- Accuracy: 0.22727272727272727\n",
    "- F1 Score: 0.563576208737499"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13f0ef1-a1f6-46ff-808c-9fab75e9555d",
   "metadata": {},
   "source": [
    "The accuracy indicates that the model correctly predicted labels for only about 22.7% of the instances in the dataset. The F1 score shows model achieves a moderate balance between precision  and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4ccc7b-823e-40bc-b639-13513d73d634",
   "metadata": {},
   "source": [
    "**Performance Metrics Table**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7ead0d-e832-425f-97da-0621eb7f6317",
   "metadata": {},
   "source": [
    "| Metric        | Precision | Recall | F1-Score | Support |\n",
    "|---------------|-----------|--------|----------|---------|\n",
    "| micro avg     | 0.28      | 0.68   | 0.40     | 31      |\n",
    "| macro avg     | 0.05      | 0.10   | 0.06     | 31      |\n",
    "| weighted avg  | 0.54      | 0.68   | 0.56     | 31      |\n",
    "| samples avg   | 0.46      | 0.77   | 0.53     | 31      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d52f1a-fd66-4b6c-a2e9-805e73f702f7",
   "metadata": {},
   "source": [
    "The micro average indicates moderate performance with an F1-score of 0.40, suggesting that the model identifies relevant instances more effectively than it avoids false positives. The macro average, with an F1-score of 0.06, highlights poor generalization across all classes, particularly less frequent ones. The weighted average provides the highest F1-score of 0.56, reflecting better performance on dominant classes. Lastly, the samples average demonstrates higher recall (0.77) and an F1-score of 0.53, indicating that the model captures many positives but struggles with precision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623a2991-3503-44e3-9d3b-d941f069357f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Support vector machine (UA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bcb68c-71f2-4910-814f-eadc96cea3a1",
   "metadata": {},
   "source": [
    "**Overall performance**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89420b57-2b7a-40aa-a75f-6185d345e139",
   "metadata": {},
   "source": [
    "- Accuracy: 0.10256410256410256\n",
    "- F1 Score: 0.1695174461223907"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c1b2c3-d54e-4142-aa92-26a6b7aa0db7",
   "metadata": {},
   "source": [
    "An accuracy of 10.26% indicates that the model correctly predicted all labels for only about 10% of the samples in the dataset. As for the CC category the F1 score shows moderate balance between precision  and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a82479-e659-4646-908a-a245a62f5f5f",
   "metadata": {},
   "source": [
    "**Performance Metrics Table**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdd05e4-babe-4334-a0ea-eeba244240dc",
   "metadata": {},
   "source": [
    "| Metric        | Precision | Recall | F1-Score | Support |\n",
    "|---------------|-----------|--------|----------|---------|\n",
    "| micro avg     | 0.30      | 0.26   | 0.28     | 151     |\n",
    "| macro avg     | 0.05      | 0.09   | 0.06     | 151     |\n",
    "| weighted avg  | 0.14      | 0.26   | 0.17     | 151     |\n",
    "| samples avg   | 0.36      | 0.47   | 0.37     | 151     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0ef01a-f7d3-46d4-9197-677097bf3d15",
   "metadata": {},
   "source": [
    "The micro average F1-score of 0.28 reflects that the model struggles to balance precision and recall when considering all labels equally. The macro average, with an F1-score of 0.06, reveals  issues in handling less frequent classes, indicating poor generalization. The weighted average F1-score of 0.17 shows better performance on dominant classes but remains weak overall. The samples average F1-score of 0.37, with a higher recall of 0.47, indicates the model captures many positives but sacrifices precision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058a6a61-9239-4eb4-b8a8-cd19d18c664a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Transformer (CC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76689e31-9047-42b7-8ac8-53aa3849a4c1",
   "metadata": {},
   "source": [
    "**Overall performance**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78d5441-9e38-4218-b4e1-57f89249d306",
   "metadata": {},
   "source": [
    "- Accuracy: 0.0\n",
    "- F1 Score: 0.6248345060324271"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bdab95-c242-4831-8f20-0c1b528e3c42",
   "metadata": {},
   "source": [
    "The model did not correcrly predict any instances. However the F1 scores shows moderare balance between precision  and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efce74d5-b774-4bed-8647-d029237e086f",
   "metadata": {},
   "source": [
    "**Performance Metrics Table**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0353af5-be80-4667-8c03-9c11e2af953f",
   "metadata": {},
   "source": [
    "| Metric        | Precision | Recall | F1-Score | Support |\n",
    "|---------------|-----------|--------|----------|---------|\n",
    "| micro avg     | 0.23      | 0.71   | 0.35     | 31      |\n",
    "| macro avg     | 0.02      | 0.05   | 0.02     | 31      |\n",
    "| weighted avg  | 0.59      | 0.71   | 0.62     | 31      |\n",
    "| samples avg   | 0.25      | 0.93   | 0.38     | 31      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e40faf-54a6-48ef-98d9-e4741bd043c8",
   "metadata": {},
   "source": [
    "The micro average F1-score of 0.35 shows moderate performance in identifying relevant instances, with a recall of 0.71 indicating the model captures many positive cases but struggles with precision (0.23). The macro average, with an F1-score of 0.02, reveals poor generalization across all classes, especially for underrepresented ones. The weighted average demonstrates better performance, with an F1-score of 0.62, indicating the model is more effective on dominant classes. The samples average F1-score of 0.38, with high recall (0.93), reflects the model's strength in detecting positive samples but at the expense of precision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af25112-b8bd-476b-9860-e169aa20ffac",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Transformer (UA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a35ee95-7dcc-4aa9-809a-035e395975f5",
   "metadata": {},
   "source": [
    "**Overall performance**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ec2f44-f8c2-417e-933f-b5b53ad41ede",
   "metadata": {},
   "source": [
    "- Accuracy: 0.0\n",
    "- F1 Score: 0.15387892530749672"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b028cbe0-ffc2-4e2c-ae18-481011fb4b06",
   "metadata": {},
   "source": [
    "The model failed to correctly predict any instance from the dataset. The F1-score of 0.15 highlights a poor balance between precision and recall, reflecting the model's struggle to handle the classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afb4398-fc90-4bca-b2da-279dbc845b28",
   "metadata": {},
   "source": [
    "**Performance Metrics Table**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a70950-d283-46d9-b99d-6bc34fdf6019",
   "metadata": {},
   "source": [
    "| Metric        | Precision | Recall | F1-Score | Support |\n",
    "|---------------|-----------|--------|----------|---------|\n",
    "| micro avg     | 0.27      | 0.28   | 0.27     | 120     |\n",
    "| macro avg     | 0.04      | 0.05   | 0.03     | 120     |\n",
    "| weighted avg  | 0.14      | 0.28   | 0.15     | 120     |\n",
    "| samples avg   | 0.29      | 0.49   | 0.31     | 120     |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cad774-89a8-409c-9939-c49244ddbbba",
   "metadata": {},
   "source": [
    "The micro average F1-score of 0.27 suggests that the model performs poorly overall, with low precision (0.27) and recall (0.28), indicating struggles in both identifying relevant instances and avoiding false positives. The macro average, with an F1-score of 0.03, highlights the modelâ€™s inability to generalize across all classes, especially for underrepresented ones. The weighted average F1-score of 0.15 demonstrates slightly better performance on dominant classes, although still weak overall. The samples average, with an F1-score of 0.31 and higher recall (0.49), indicates the model captures many positive samples but sacrifices precision."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
