{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03d8abb8-7d1a-4152-b272-47d3cd4d172b",
   "metadata": {},
   "source": [
    "# Quantitative analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d194a6c-3112-4691-b040-14acf0ed3948",
   "metadata": {},
   "source": [
    "For a better interpretability, the quantitative analysis of the implemented models is divided into two macro-categories: Climate Change ( CC) and War in Ukraine (UA)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bf9a61-1083-4fcc-b6a9-22084b0694aa",
   "metadata": {},
   "source": [
    "### Long short-term memory (CC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51894d5d-0e61-4e02-88fd-aa02ae991f34",
   "metadata": {},
   "source": [
    "**Overall performance**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409ecbd8-6c92-49b4-891c-b94424767cc3",
   "metadata": {},
   "source": [
    "- Accuracy: 0.0\n",
    "- F1 Score: 0.6172443732051025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b460c9-eff3-45d0-bbc2-1bb689c435b8",
   "metadata": {},
   "source": [
    "An accuracy of 0.0 in a model's performance indicates that the model did not correctly predict any instance in the dataset according to the true labels.\n",
    "The F1 score shows that the model achieves a moderate balance between precision  and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e338f08-7b1f-4ae3-a0e8-23d2fafdc991",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "**Performance Metrics Table**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e364f5-5cc5-48d2-b770-dbd239b87fbf",
   "metadata": {},
   "source": [
    "| Metric        | Precision | Recall | F1-Score | Support |\n",
    "|---------------|-----------|--------|----------|---------|\n",
    "| micro avg     | 0.28      | 0.68   | 0.40     | 31      |\n",
    "| macro avg     | 0.02      | 0.04   | 0.02     | 31      |\n",
    "| weighted avg  | 0.59      | 0.68   | 0.62     | 31      |\n",
    "| samples avg   | 0.31      | 0.92   | 0.45     | 31      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b038b7e0-c791-4ed4-90d5-df3b7a44b3be",
   "metadata": {},
   "source": [
    "The micro average shows moderate performance, with a precision of 0.28, recall of 0.68, and an F1-score of 0.40. This suggests the model is somewhat effective at identifying relevant instances but struggles to avoid false positives. The macro average indicates poor performance across classes, with very low precision, recall, and F1-score (all around 0.02). \n",
    "The weighted average provides the best results among the metrics, with precision at 0.59, recall at 0.68, and an F1-score of 0.62. This suggests the model performs well on classes with more instances, adjusting for class frequency. Lastly, the samples average reflects high recall (0.92) but lower precision (0.31) and an F1-score of 0.45, indicating that while the model captures most positive samples, it also misclassifies many instances as positive incorrectly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e31c90-08a0-4a2b-8ad0-1130da2ecf1b",
   "metadata": {},
   "source": [
    "### Long short-term memory (UA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e7f392-043f-4557-9e57-b23b617b304d",
   "metadata": {},
   "source": [
    "**Overall performance**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51437e8d-f965-4a0c-ad0e-15ecd5b1bead",
   "metadata": {},
   "source": [
    "- Accuracy: 0.0\n",
    "- F1 Score: 0.12845804988662132 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b70fe14-0fb6-47d2-acf4-e2ac11645b9e",
   "metadata": {},
   "source": [
    "The model did not produce any correct prediction. The F1 score suggests a very low balance betweeen precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f743f21-8145-4e4a-8463-cf360d760969",
   "metadata": {},
   "source": [
    "**Performance Metrics Table**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85f79cc-b149-4115-b429-5deb2d106a99",
   "metadata": {},
   "source": [
    "| Metric        | Precision | Recall | F1-Score | Support |\n",
    "|---------------|-----------|--------|----------|---------|\n",
    "| micro avg     | 0.32      | 0.23   | 0.27     | 120     |\n",
    "| macro avg     | 0.02      | 0.04   | 0.02     | 120     |\n",
    "| weighted avg  | 0.09      | 0.23   | 0.13     | 120     |\n",
    "| samples avg   | 0.32      | 0.46   | 0.33     | 120     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844c5070-07da-4b10-9c2d-ae5f8d0506e0",
   "metadata": {},
   "source": [
    "The model's micro average F1-score is 0.27, indicating low performance when evaluating all instances equally. Precision is slightly higher at 0.32, but recall remains low at 0.23, suggesting the model struggles with identifying relevant instances. The macro average F1-score of 0.02 highlights severe difficulty in generalizing across all classes, particularly for less-represented ones. The weighted average F1-score of 0.13 reflects better performance on dominant classes, though still quite poor overall. At the sample level, the model achieves an F1-score of 0.33, with higher recall (0.46), showing it captures more positives but sacrifices precision. Overall, the model demonstrates hight limitations, particularly with class imbalances and underrepresented categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8051096f-8276-44d9-876b-95166727c908",
   "metadata": {},
   "source": [
    "### Support vector machine (CC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0de024-b535-4286-89d8-df9f67eb35b6",
   "metadata": {},
   "source": [
    "**Overall performance**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d8ecd6-2264-4a7b-afe8-10c5cc1e5045",
   "metadata": {},
   "source": [
    "- Accuracy: 0.22727272727272727\n",
    "- F1 Score: 0.563576208737499"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13f0ef1-a1f6-46ff-808c-9fab75e9555d",
   "metadata": {},
   "source": [
    "The accuracy indicates that the model correctly predicted labels for only about 22.7% of the instances in the dataset. The F1 score shows model achieves a moderate balance between precision  and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4ccc7b-823e-40bc-b639-13513d73d634",
   "metadata": {},
   "source": [
    "**Performance Metrics Table**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7ead0d-e832-425f-97da-0621eb7f6317",
   "metadata": {},
   "source": [
    "| Metric        | Precision | Recall | F1-Score | Support |\n",
    "|---------------|-----------|--------|----------|---------|\n",
    "| micro avg     | 0.28      | 0.68   | 0.40     | 31      |\n",
    "| macro avg     | 0.05      | 0.10   | 0.06     | 31      |\n",
    "| weighted avg  | 0.54      | 0.68   | 0.56     | 31      |\n",
    "| samples avg   | 0.46      | 0.77   | 0.53     | 31      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d52f1a-fd66-4b6c-a2e9-805e73f702f7",
   "metadata": {},
   "source": [
    "The micro average indicates moderate performance with an F1-score of 0.40, suggesting that the model identifies relevant instances more effectively than it avoids false positives. The macro average, with an F1-score of 0.06, highlights poor generalization across all classes, particularly less frequent ones. The weighted average provides the highest F1-score of 0.56, reflecting better performance on dominant classes. Lastly, the samples average demonstrates higher recall (0.77) and an F1-score of 0.53, indicating that the model captures many positives but struggles with precision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623a2991-3503-44e3-9d3b-d941f069357f",
   "metadata": {},
   "source": [
    "### Support vector machine (UA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bcb68c-71f2-4910-814f-eadc96cea3a1",
   "metadata": {},
   "source": [
    "**Overall performance**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89420b57-2b7a-40aa-a75f-6185d345e139",
   "metadata": {},
   "source": [
    "- Accuracy: 0.10256410256410256\n",
    "- F1 Score: 0.1695174461223907"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c1b2c3-d54e-4142-aa92-26a6b7aa0db7",
   "metadata": {},
   "source": [
    "An accuracy of 10.26% indicates that the model correctly predicted all labels for only about 10% of the samples in the dataset. As for the CC category the F1 score shows moderate balance between precision  and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a82479-e659-4646-908a-a245a62f5f5f",
   "metadata": {},
   "source": [
    "**Performance Metrics Table**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdd05e4-babe-4334-a0ea-eeba244240dc",
   "metadata": {},
   "source": [
    "| Metric        | Precision | Recall | F1-Score | Support |\n",
    "|---------------|-----------|--------|----------|---------|\n",
    "| micro avg     | 0.30      | 0.26   | 0.28     | 151     |\n",
    "| macro avg     | 0.05      | 0.09   | 0.06     | 151     |\n",
    "| weighted avg  | 0.14      | 0.26   | 0.17     | 151     |\n",
    "| samples avg   | 0.36      | 0.47   | 0.37     | 151     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0ef01a-f7d3-46d4-9197-677097bf3d15",
   "metadata": {},
   "source": [
    "The micro average F1-score of 0.28 reflects that the model struggles to balance precision and recall when considering all labels equally. The macro average, with an F1-score of 0.06, reveals  issues in handling less frequent classes, indicating poor generalization. The weighted average F1-score of 0.17 shows better performance on dominant classes but remains weak overall. The samples average F1-score of 0.37, with a higher recall of 0.47, indicates the model captures many positives but sacrifices precision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058a6a61-9239-4eb4-b8a8-cd19d18c664a",
   "metadata": {},
   "source": [
    "### Transformer (CC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76689e31-9047-42b7-8ac8-53aa3849a4c1",
   "metadata": {},
   "source": [
    "**Overall performance**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78d5441-9e38-4218-b4e1-57f89249d306",
   "metadata": {},
   "source": [
    "- Accuracy: 0.0\n",
    "- F1 Score: 0.6248345060324271"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bdab95-c242-4831-8f20-0c1b528e3c42",
   "metadata": {},
   "source": [
    "The model did not correcrly predict any instances. However the F1 scores shows moderare balance between precision  and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efce74d5-b774-4bed-8647-d029237e086f",
   "metadata": {},
   "source": [
    "**Performance Metrics Table**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0353af5-be80-4667-8c03-9c11e2af953f",
   "metadata": {},
   "source": [
    "| Metric        | Precision | Recall | F1-Score | Support |\n",
    "|---------------|-----------|--------|----------|---------|\n",
    "| micro avg     | 0.23      | 0.71   | 0.35     | 31      |\n",
    "| macro avg     | 0.02      | 0.05   | 0.02     | 31      |\n",
    "| weighted avg  | 0.59      | 0.71   | 0.62     | 31      |\n",
    "| samples avg   | 0.25      | 0.93   | 0.38     | 31      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e40faf-54a6-48ef-98d9-e4741bd043c8",
   "metadata": {},
   "source": [
    "The micro average F1-score of 0.35 shows moderate performance in identifying relevant instances, with a recall of 0.71 indicating the model captures many positive cases but struggles with precision (0.23). The macro average, with an F1-score of 0.02, reveals poor generalization across all classes, especially for underrepresented ones. The weighted average demonstrates better performance, with an F1-score of 0.62, indicating the model is more effective on dominant classes. The samples average F1-score of 0.38, with high recall (0.93), reflects the model's strength in detecting positive samples but at the expense of precision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af25112-b8bd-476b-9860-e169aa20ffac",
   "metadata": {},
   "source": [
    "### Transformer (UA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a35ee95-7dcc-4aa9-809a-035e395975f5",
   "metadata": {},
   "source": [
    "**Overall performance**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ec2f44-f8c2-417e-933f-b5b53ad41ede",
   "metadata": {},
   "source": [
    "- Accuracy: 0.0\n",
    "- F1 Score: 0.15387892530749672"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b028cbe0-ffc2-4e2c-ae18-481011fb4b06",
   "metadata": {},
   "source": [
    "The model failed to correctly predict any instance from the dataset. The F1-score of 0.15 highlights a poor balance between precision and recall, reflecting the model's struggle to handle the classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afb4398-fc90-4bca-b2da-279dbc845b28",
   "metadata": {},
   "source": [
    "**Performance Metrics Table**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a70950-d283-46d9-b99d-6bc34fdf6019",
   "metadata": {},
   "source": [
    "| Metric        | Precision | Recall | F1-Score | Support |\n",
    "|---------------|-----------|--------|----------|---------|\n",
    "| micro avg     | 0.27      | 0.28   | 0.27     | 120     |\n",
    "| macro avg     | 0.04      | 0.05   | 0.03     | 120     |\n",
    "| weighted avg  | 0.14      | 0.28   | 0.15     | 120     |\n",
    "| samples avg   | 0.29      | 0.49   | 0.31     | 120     |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cad774-89a8-409c-9939-c49244ddbbba",
   "metadata": {},
   "source": [
    "The micro average F1-score of 0.27 suggests that the model performs poorly overall, with low precision (0.27) and recall (0.28), indicating struggles in both identifying relevant instances and avoiding false positives. The macro average, with an F1-score of 0.03, highlights the modelâ€™s inability to generalize across all classes, especially for underrepresented ones. The weighted average F1-score of 0.15 demonstrates slightly better performance on dominant classes, although still weak overall. The samples average, with an F1-score of 0.31 and higher recall (0.49), indicates the model captures many positive samples but sacrifices precision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6390150-dd95-4bdc-a596-25a108a286ff",
   "metadata": {},
   "source": [
    "### BERT (CC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a227b36-fc0a-4329-b7c7-b56632452ca7",
   "metadata": {},
   "source": [
    "**Overall performance**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcba358-d27f-4052-a009-3312be8b72a2",
   "metadata": {},
   "source": [
    "- Accuracy: 0.4444444444444444\n",
    "- F1 Score: 0.38327092272694535"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92453b5b-59ed-4dfe-aff7-383012c9921c",
   "metadata": {},
   "source": [
    "The accuracy of 0.44 indicates that the model correctly predicted approximately 44% of the instances in the dataset. The F1 score of 0.38 indicates that the model has limited effectiveness in balancing precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f557f5-25ea-4214-8824-fe70aeedd190",
   "metadata": {},
   "source": [
    "**Performance Metrics Table**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a3b9af-f453-4a3c-b696-b88edfb9e5ff",
   "metadata": {},
   "source": [
    "| Metric        | Precision | Recall | F1-Score | Support |\n",
    "|---------------|-----------|--------|----------|---------|\n",
    "| micro avg     | 0.66      | 0.36   | 0.46     | 149     |\n",
    "| macro avg     | 0.16      | 0.11   | 0.12     | 149     |\n",
    "| weighted avg  | 0.48      | 0.36   | 0.38     | 149     |\n",
    "| samples avg   | 0.71      | 0.58   | 0.61     | 149     | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cdd649-556c-452b-ac4f-3a7ae6814e80",
   "metadata": {},
   "source": [
    "The micro average F1-score of 0.46 indicates moderate overall performance, with precision at 0.66 but lower recall at 0.36, suggesting the model is better at avoiding false positives than capturing all relevant instances. The macro average F1-score of 0.12 highlights significant struggles in handling underrepresented classes, showing poor generalization across all categories. The weighted average F1-score of 0.38 reflects slightly better performance on more dominant classes, though it remains suboptimal. The samples average F1-score of 0.61, with higher precision (0.71) and recall (0.58), suggests the model is relatively better at handling multiple-label instances but still has room for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bdb7d6-fa73-4b42-99ef-e9e19b905c24",
   "metadata": {},
   "source": [
    "### BERT (UA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c8e764-b4cd-4c78-b56f-09c29c2702b6",
   "metadata": {},
   "source": [
    "**Overall performance**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5e81b6-dca6-4783-bdab-d82d98a1c21d",
   "metadata": {},
   "source": [
    "- Accuracy: 0.5111111111111111\n",
    "- F1 Score: 0.2652014652014652"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23020d3d-8189-4240-8aa8-259f019eea68",
   "metadata": {},
   "source": [
    "The accuracy of 0.51 indicates that the model correctly predicts just over half of the instances in the dataset, which is not better than random guessing. However, the F1 score of 0.27 reveals poor balance between precision and recall, suggesting the model is again struggles to effectively capture relevant instances while avoiding false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627b65e4-ea06-4ea7-a9bb-3dfc6c178715",
   "metadata": {},
   "source": [
    "**Performance Metrics Table**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55a19f3-2274-415a-86b9-cc5c10ed2380",
   "metadata": {},
   "source": [
    "| Metric        | Precision | Recall | F1-Score | Support |\n",
    "|---------------|-----------|--------|----------|---------|\n",
    "| micro avg     | 0.59      | 0.30   | 0.40     | 116     |\n",
    "| macro avg     | 0.07      | 0.05   | 0.05     | 116     |\n",
    "| weighted avg  | 0.29      | 0.30   | 0.27     | 116     |\n",
    "| samples avg   | 0.64      | 0.58   | 0.59     | 116     |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c54a76-5faf-4363-b918-f492e38e929d",
   "metadata": {},
   "source": [
    "The micro average F1-score of 0.40 shows moderate overall performance, with higher precision (0.59) than recall (0.30), indicating the model identifies some relevant instances but misses many. The macro average F1-score of 0.05 highlights the model's inability to handle underrepresented classes effectively. The weighted average F1-score of 0.27 shows slightly better performance on dominant classes, though still limited. The samples average F1-score of 0.59, with higher precision (0.64) and recall (0.58), suggests the model performs relatively better in multi-label instances but has significant space for improvement overall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8e5f81-ae48-4923-840e-e8159f2c1513",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3029781-8915-49d1-a7d6-d1c672c93b07",
   "metadata": {},
   "source": [
    "The quantitative analysis has revealed that BERT is the best-performing model for both CC and UA tasks. Regarding CC, BERT achieved the highest micro F1-score (0.46), was quite strong in weighted F1-score (0.38), and attained a good overall balance with a score of 0.44 in terms of accuracy, which means it can manage the dominant classes. BERT also attained the same micro F1-score of 0.40 for UA, but with better balanced precision and recall compared to other models. It also has the highest accuracy at 0.51, proving good capabilities in nuanced and imbalanced narrative structures."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
